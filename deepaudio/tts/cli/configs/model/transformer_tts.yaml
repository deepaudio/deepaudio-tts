_target_: deepaudio.tts.models.transformer_tts.model.TransformerTTSModel

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.00001

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

transformer_loss:
  _target_: deepaudio.tts.models.transformer_tts.loss.TransformerLoss
  use_masking: True
  use_weighted_masking: False
  bce_pos_weight: 20.0

atten_criterion:
  _target_: deepaudio.tts.models.transformer_tts.loss.GuidedMultiHeadAttentionLoss
  sigma: 0.4
  alpha: 1.0
  reset_always: True


model:
  _target_: deepaudio.tts.models.transformer_tts.transformer.Transformer
  embed_dim: 0           # embedding dimension in encoder prenet
  eprenet_conv_layers: 0 # number of conv layers in encoder prenet
  # if set to 0, no encoder prenet will be used
  eprenet_conv_filts: 0  # filter size of conv layers in encoder prenet
  eprenet_conv_chans: 0  # number of channels of conv layers in encoder prenet
  dprenet_layers: 2      # number of layers in decoder prenet
  dprenet_units: 256     # number of units in decoder prenet
  adim: 512              # attention dimension
  aheads: 8              # number of attention heads
  elayers: 6             # number of encoder layers
  eunits: 1024           # number of encoder ff units
  dlayers: 6             # number of decoder layers
  dunits: 1024           # number of decoder ff units
  positionwise_layer_type: conv1d  # type of position-wise layer
  positionwise_conv_kernel_size: 1 # kernel size of position wise conv layer
  postnet_layers: 5                # number of layers of postnset
  postnet_filts: 5                 # filter size of conv layers in postnet
  postnet_chans: 256               # number of channels of conv layers in postnet
  use_scaled_pos_enc: True         # whether to use scaled positional encoding
  encoder_normalize_before: True   # whether to perform layer normalization before the input
  decoder_normalize_before: True   # whether to perform layer normalization before the input
  reduction_factor: 1              # reduction factor
  init_type: xavier_uniform        # initialization type
  init_enc_alpha: 1.0              # initial value of alpha of encoder scaled position encoding
  init_dec_alpha: 1.0              # initial value of alpha of decoder scaled position encoding
  eprenet_dropout_rate: 0.0        # dropout rate for encoder prenet
  dprenet_dropout_rate: 0.5        # dropout rate for decoder prenet
  postnet_dropout_rate: 0.5        # dropout rate for postnet
  transformer_enc_dropout_rate: 0.1                # dropout rate for transformer encoder layer
  transformer_enc_positional_dropout_rate: 0.1     # dropout rate for transformer encoder positional encoding
  transformer_enc_attn_dropout_rate: 0.1           # dropout rate for transformer encoder attention layer
  transformer_dec_dropout_rate: 0.1                # dropout rate for transformer decoder layer
  transformer_dec_positional_dropout_rate: 0.1     # dropout rate for transformer decoder positional encoding
  transformer_dec_attn_dropout_rate: 0.1           # dropout rate for transformer decoder attention layer
  transformer_enc_dec_attn_dropout_rate: 0.1       # dropout rate for transformer encoder-decoder attention layer
  num_heads_applied_guided_attn: 2                 # number of heads to apply guided attention loss
  num_layers_applied_guided_attn: 2                # number of layers to apply guided attention loss

